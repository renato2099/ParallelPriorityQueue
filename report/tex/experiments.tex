\section{Experimental Results}
\label{sec:exp}
%CentOS release 6.5--icpc (ICC) 15.0.0 20140723
%Ubuntu 14.04--icpc (ICC) 15.0.1 20141023
\subsection{Evaluation on Workloads}
The experiments were conducted on two different systems. First, a CPU-based system equipped with an Intel Xeon E3-1245 processor running 4 cores at 3.4\,Ghz and 24\,GB main memory and using Ubuntu 14.04 as operating system. Second a MIC, the Intel Xeon Phi 7120P which has 60 cores with 4 hyper-threads, each clocked at 1.23\,Ghz. It has 16\,GB main memory with a bandwidth of 352\,GB/s using CentOS 6.5 as host operating system. The code was compiled using Intel C++ compiler (\textit{icpc}) version 15.0.0 for both systems. To allow a comparison, the experiment parameters used were the same on both platforms. For each experiment five iterations were averaged. %and their standard deviation was computed.

Three types of workload were executed, \textit{mixed}, \textit{push} and \textit{pop}. For the mixed and pop workloads, the concurrent priority queue was prepopulated with 1000$\times$threads for the former and 10\,Mio. elements for the latter. Prepopulation was not included in the measured runtime. The \textit{mixed} workload chooses between a \textit{push} or \textit{pop} operation with a probability of 50\%. The number of threads is increased from 1 to 240, in steps of 20, while the number of operations per threads stays constant.

\subsubsection{Evaluation on Xeon Haswell}
\begin{figure*}[t]
	\centering
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		%\includegraphics[width=\textwidth]{../plots/i7_mixed/runtime_mixed_i7}
		\input{../plots/xeon_mixed/xeon_mixed}
		\caption{Mixed workload}
		\label{fig:xeon_mixed}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		\input{../plots/xeon_push/xeon_push}
		\caption{Push workload}
		\label{fig:xeon_push}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		%\includegraphics[width=\textwidth]{../plots/i7_pop/runtime_pop_i7}
		\input{../plots/xeon_pop/xeon_pop}
		\caption{Pop workload}
		\label{fig:xeon_pop}
	\end{subfigure}
	\caption{Runtime for different workloads executed on a Xeon E3-1245 while varying the number of threads}
	\label{fig:eval_i7}
\end{figure*}
Fig.~\ref{fig:eval_i7} shows the run-times for executing the three workloads on a Xeon E3-1245 system. All variants are performing and behaving similar in the \textit{mixed} and \textit{push} workload, except for the lock-based implementation which struggles more to scale with the number of threads. Our lock-free implementation is very close to baseline implementations and shows similar behavior when increasing the number of threads. In the \textit{pop} workload, all four data structures behave very similar.
The lock-free implementation of the pop operation is done in $\Theta(1)$ and lazy-deletion shows the best runtime by a small margin.

\subsubsection{Evaluation on Xeon Phi}
\begin{figure*}[t]
	\centering
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		%\includegraphics[width=\textwidth]{../plots/xp_mixed/runtime_mixed}
		\input{../plots/xp_mixed/xp_mixed}
		\caption{Mixed workload}
		\label{fig:xp_mixed}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		%\includegraphics[width=\textwidth]{../plots/xp_push/runtime_push}
		\input{../plots/xp_push/xp_push}
		\caption{Push workload}
		\label{fig:xp_push}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		%\includegraphics[width=\textwidth]{../plots/xp_pop/runtime_pop}
		\input{../plots/xp_pop/xp_pop}
		\caption{Pop workload}
		\label{fig:xp_pop}
	\end{subfigure}
	\caption{Runtime for different workloads executed on a Xeon Phi while varying the number of threads}
	\label{fig:eval_xp}
\end{figure*}
The same workloads were also run on Xeon Phi. The measured run-times are plotted in Fig.~\ref{fig:eval_xp}.
In the mixed workload the TBB and the lock-free concurrent priority queues behave similarly well. The other two implementations show clearly a longer runtime.
Looking at the \textit{push} wrt. \textit{pop} workload it is visible that the longer runtime in the lock-based implementation is originated by the poor performance from the \textit{push} operation. For the STD implementation we used a lock-based wrapper which leads to a long runtime for the \textit{pop} operation.
The TBB implementation behaves well in all three workloads.
Our lock-free implementation shows almost the same behavior with a slightly longer runtime.

Comparing the runtime of the two platforms shows that the same workload is about one magnitude faster on Xeon CPU.

\subsection{Impact of cache-invalidation}
\begin{figure}[t]
	\centering
	%\includegraphics[width=0.9\columnwidth]{../plots/comp_contains/runtime_contains}
	\input{../plots/comp_contains/comp_contains}
	\caption{Cache-invalidation assessment}
	\label{fig:comp_contains}
\end{figure}
The previous experiments show that the same workload performs roughly a magnitude better on a Xeon CPU than on Xeon Phi.
Despite the fact that Xeon Phi has many more cores and a higher memory bandwidth.
There are multiple reasons for these, e.g. cache structure, memory allocation cost, and other problems.
Therefore an experiment was designed to check how Xeon Phi performs, in comparison to other architectures when cache-invalidations are not occurring.
This consists of prepopulating our lock-free concurrent priority queue with 10 million elements, then every thread performs 100'000 element lookups.
Thereby the structure of the priority queue is unaltered which means no cache-line belonging to the concurrent priority queue should get invalidated.
This experiment was executed with 1 up to 240 threads. The run-times from executing this experiment on an Intel Core i7-3820, a Xeon CPU and the Xeon Phi, are plotted in Fig.~\ref{fig:comp_contains}.
% FIXME can you improve the second part of the following sentence. It should become 2 sentences, instead of 1.
It can be noted that the runtime on Xeon Phi stays stable up to 60 threads equaling the number of cores. Even with more threads its runtime increases only slightly.
Whereas on the other two systems, the execution time increases linearly.
The results show clearly that the invalidation of cache-lines and their consequences have a major impact on the performance on Xeon Phi.
While in the previous experiments it showed a linear increase in execution time when scaling up, in this case the runtime stays almost constant which is close to ideal.

%Also rephrase reson why Xeon Phi is not doing well
This finding is not new, other researchers~\cite{ramos-hoefler-cc-modeling} have shown that a drawback when using a Xeon Phi is that any cache-line invalidation might lead to many cores ending up with invalid cache lines . This is due to fact that it uses a distributed tag directory. Thus, the more hardware cores are used on a concurrent application the probability that at least two threads share a cache line is higher. These dirty cache lines produce expected overhead due to cache invalidations.

Implementing a truly concurrent data structure is a challenging task for many reasons. For instance, even though our implementation is lock-free, it may not be starvation-free. There could be a thread A that when going through the lowest level of the skip list searching for the next un-marked node (i.e. logically undeleted) gets always outrun by some other thread B. This means that thread A can fail repeatedly if the others threads always succeed. Moreover, thread contention may happen if many threads try to logically delete a node (i.e. mark a node). Only one thread will succeed and all the other unsuccessful ones will race to mark
the next available node.\\


\subsection{Operational intensity in different microarchitectures}
% recap of operational intensity
Operational intensity is defined as the ratio of the number of instructions executed to the number of memory accesses~\cite{roger1996science}. If there exist many instructions per memory access, then the program is considered to have a high computational intensity i.e. compute bounded. On the other hand, if there are a small number of instructions are executed per memory access, then the program is considered to have a low computational intensity i.e. memory bounded.

% why we think it matters in our case
Our project goal was to design a simple, yet effective, concurrent priority queue. Thus, we expected to have an operational intensity dominated mainly by the number of memory accesses, and aimed to improve this. Having to move data around, has a different impact on different CPU architectures. We will describe and explain how our data structure behaves on Intel Haswell microarchitecture (Intel Core i7-4558U) and on Sandy BridgeE microarchitecture (Intel Core i7-3820K). 

% Differences between these two microarchitectures
The Intel Haswell microarchiteture is the successor of Ivy Bridge which in turn is the successor of Sandy Bridge. They have several differences but they also share many commonalities. One of the biggest change is the enhancement done on cache level operations. These changes are summarized in \tablename~\ref{tab:haswell_ivy}~\cite{ijcsit2013040321, microarchitecture, haswell_arch}.

\begin{table}[ht]
\footnotesize
\begin{tabular}{|l|l|l|ll}
\cline{1-3}
\multicolumn{1}{|c|}{\textbf{Metric}} & \multicolumn{1}{c|}{\textbf{Sandy BridgeE}} & \multicolumn{1}{c|}{\textbf{Haswell}} &  &  \\ \cline{1-3}
L1 Load Bandwidth                     & 32 Bytes/cycle                           & 64 Bytes/cycle                        &  &  \\ \cline{1-3}
L1 Store bandwidth                    & 16 Bytes/cycle                           & 32 Bytes/cycle                        &  &  \\ \cline{1-3}
L2 Bandwidth to L1                    & 32 Bytes/cycle                           & 64 Bytes/cycle                        &  &  \\ \cline{1-3}
L2 Unified TLB                        & 4K:512, 4-way                            & 4k+2M shared: 1024, 8-way             &  &  \\ \cline{1-3}
\end{tabular}
\caption{Cache operation differences between Intel Haswell and Intel SandyBridgeE}
\label{tab:haswell_ivy}
\end{table}

% describe cache structure
In addition to core cache size, latency, and bandwidth improvements, the Intel Haswell microarchitecture has also improved its ICache prefetch algorithms, and the way it handles conflicts. It uses hardware transactions i.e. it uses hardware to keep track of which cache lines have been read from and which have been written to. L1 cache tracks addresses read/written from/to respectively in the transactional region and it may evict address but without loss of tracking. Data conflicts occur if at least one request is doing a write, but it is detected at cache line granularity using existing cache coherence protocol~\cite{rajwar_qconsf2012,dk_haswell}.

While running our priority queue benchmark on these two different architectures, we observed different behaviors. The running times when using an Intel IvyBridge processor dramatically increase due to a higher number of instructions and cache misses. Everytime we need to perform an insertion, we first have to search for the adequate place within the skip list. The average of the skip list nodes fit in a 64-byte cache line but the ones containing pointers in upper levels do not. On the other hand, when we used an Intel Haswell processor, running times were much less than in the Sandy BridgeE processor. This is mainly due to the improvements done on cache operations. In this case, our data structure can take advantage of such improvements by loading more skip list nodes into the caches that can also be used by other threads.

% explain data + graph + core architecture
\begin{figure}[t]
	\centering
  	%\includegraphics[scale=0.3]{../plots/haswell-ivybridge/haswell_ivybridge.pdf}
	\input{../plots/haswell-ivybridge/haswell_ivy}
	\caption{Op. Intensity in Intel Haswell and Intel Sandy BridgeE microarchitectures}
	\label{fig:haswell_ivybridge}
\end{figure}

Fig.~\ref{fig:haswell_ivybridge} shows how operational intensity behaves when running different amounts of insert operations over such micro-architectures. It can be noted that when performing a small number of operations, our data structure is CPU bounded on an Sandy BridgeE processor, but memory bounded on a Haswell processor. This is because in the former we have a small number of cache-misses against a really high number of instructions whether in the latter we observed a low operational intensity because we need less number of instructions for performing such tasks. When we increase the number of operations, the Sandy BridgeE processor gets many more cache-misses compared to the Haswell one. Thus, in the former one our data structure becomes memory bounded and in the latter one CPU bounded.
